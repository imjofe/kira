#include <iostream>
void llama_init_backend(bool use_numa) { std::cout << "llama.cpp backend initialized." << std::endl; }